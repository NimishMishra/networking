# -*- coding: utf-8 -*-
"""IDS-google-colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qn5sW78amnthr7pml9ZySVF9_KZE8YFP
"""

# from google.colab import drive

# drive.mount('drive')

import pandas as pd
import numpy as np
from sklearn import metrics
from scipy.stats import zscore
from keras.utils import get_file
import io
#import requests
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn import metrics
from keras.models import Sequential, load_model
from keras.layers import Dense, Activation
from keras.callbacks import EarlyStopping, ModelCheckpoint


# downloading the data
try:
    path = get_file('kddcup.data_10_percent.gz', origin='http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz')
except:
    print('Error downloading')
    

df = pd.read_csv(path, header=None) # reads the CSV into a pandas dataframe

print("Read {} rows.".format(len(df))) # prints the number of rows read in the dataset

df.dropna(inplace=True,axis=1)  # drops the columns having null values in their cells

# The following description of data is taken from https://kdd.ics.uci.edu/databases/kddcup99/task.html

df.columns = [
    'duration',                 # the duration of the connection
    'protocol_type',            # TCP, UDP, or ICMP? UDP = user datagram protocol, transmission control protocol, internet control message protocol
    'service',                  # network service on the destination, e.g., http, telnet, etc. 
    'flag',                     # normal or error status of the connection 
    'src_bytes',                # number of data bytes from source to destination 
    'dst_bytes',                # number of data bytes from destination to source 
    'land',                     # 1 if connection is from/to the same host/port; 0 otherwise 
    'wrong_fragment',           # number of ``wrong'' fragments 
    'urgent',                   # number of urgent packets
    'hot',                      # number of ``hot'' indicators
    'num_failed_logins',        # number of failed login attempts 
    'logged_in',                # 1 if successfully logged in; 0 otherwise 
    'num_compromised',          # number of ``compromised'' conditions 
    'root_shell',               # 	1 if root shell is obtained; 0 otherwise 
    'su_attempted',             # 	1 if ``su root'' command attempted; 0 otherwise 
    'num_root',                 # 	number of ``root'' accesses 
    'num_file_creations',       # 	number of file creation operations 
    'num_shells',               # 	number of shell prompts 
    'num_access_files',         # 	number of operations on access control files 
    'num_outbound_cmds',        #   number of outbound commands in an ftp session 
    'is_host_login',            #   1 if the login belongs to the ``hot'' list; 0 otherwise 
    'is_guest_login',           #   1 if the login is a ``guest''login; 0 otherwise 
    'count',                    # number of connections to the same host as the current connection in the past two seconds 
    
    'srv_count',                #   number of connections to the same service as the current connection in the past two seconds
    'serror_rate',              #   % of connections that have ``SYN'' errors 
    'srv_serror_rate',          #   % of connections that have ``SYN'' errors
    'rerror_rate',              #   % of connections that have ``REJ'' errors
    'srv_rerror_rate',          #   % of connections that have ``REJ'' errors 
    'same_srv_rate',            #   % of connections to the same service
    'diff_srv_rate',            #   % of connections to different services 
    'srv_diff_host_rate',       #   % of connections to different hosts
    'dst_host_count',
    'dst_host_srv_count',
    'dst_host_same_srv_rate',
    'dst_host_diff_srv_rate',
    'dst_host_same_src_port_rate',
    'dst_host_srv_diff_host_rate',
    'dst_host_serror_rate',
    'dst_host_srv_serror_rate',
    'dst_host_rerror_rate',
    'dst_host_srv_rerror_rate',
    'outcome'
]


# display 5 rows
df[0:5]

# The following function normalizes the input features
def encode_numeric_zscore(df, name, mean=None, sd=None):
    if mean is None:
        mean = df[name].mean()

    if sd is None:
        sd = df[name].std()

    df[name] = (df[name] - mean) / sd
    
# for all categorical features, this function performs one-hot encoding
def encode_text_dummy(df, name):
    dummies = pd.get_dummies(df[name])
    for x in dummies.columns:
        dummy_name = f"{name}-{x}"
        df[dummy_name] = dummies[x]
    df.drop(name, axis=1, inplace=True)

# based on whether the feature is continuous or categorical, this either normalizes it or hot encodes it
encode_numeric_zscore(df, 'duration')
encode_text_dummy(df, 'protocol_type')
encode_text_dummy(df, 'service')
encode_text_dummy(df, 'flag')
encode_numeric_zscore(df, 'src_bytes')
encode_numeric_zscore(df, 'dst_bytes')
encode_text_dummy(df, 'land')
encode_numeric_zscore(df, 'wrong_fragment')
encode_numeric_zscore(df, 'urgent')
encode_numeric_zscore(df, 'hot')
encode_numeric_zscore(df, 'num_failed_logins')
encode_text_dummy(df, 'logged_in')
encode_numeric_zscore(df, 'num_compromised')
encode_numeric_zscore(df, 'root_shell')
encode_numeric_zscore(df, 'su_attempted')
encode_numeric_zscore(df, 'num_root')
encode_numeric_zscore(df, 'num_file_creations')
encode_numeric_zscore(df, 'num_shells')
encode_numeric_zscore(df, 'num_access_files')
encode_numeric_zscore(df, 'num_outbound_cmds')
encode_text_dummy(df, 'is_host_login')
encode_text_dummy(df, 'is_guest_login')
encode_numeric_zscore(df, 'count')
encode_numeric_zscore(df, 'srv_count')
encode_numeric_zscore(df, 'serror_rate')
encode_numeric_zscore(df, 'srv_serror_rate')
encode_numeric_zscore(df, 'rerror_rate')
encode_numeric_zscore(df, 'srv_rerror_rate')
encode_numeric_zscore(df, 'same_srv_rate')
encode_numeric_zscore(df, 'diff_srv_rate')
encode_numeric_zscore(df, 'srv_diff_host_rate')
encode_numeric_zscore(df, 'dst_host_count')
encode_numeric_zscore(df, 'dst_host_srv_count')
encode_numeric_zscore(df, 'dst_host_same_srv_rate')
encode_numeric_zscore(df, 'dst_host_diff_srv_rate')
encode_numeric_zscore(df, 'dst_host_same_src_port_rate')
encode_numeric_zscore(df, 'dst_host_srv_diff_host_rate')
encode_numeric_zscore(df, 'dst_host_serror_rate')
encode_numeric_zscore(df, 'dst_host_srv_serror_rate')
encode_numeric_zscore(df, 'dst_host_rerror_rate')
encode_numeric_zscore(df, 'dst_host_srv_rerror_rate')


# remove all the columns having few null values
df.dropna(inplace=True,axis=1)
df[0:5]

x_columns = df.columns.drop('outcome') # drop the result column from the train set
x = df[x_columns].values # converting the dataframe into values
dummies = pd.get_dummies(df['outcome']) # converting into one-hot encoding
outcomes = dummies.columns
num_classes = len(outcomes)
y = dummies.values

df.groupby('outcome')['outcome'].count()


# performing a 25 % test split (leaving the rest for the train test)
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.25, random_state=42)


model = Sequential() # the Keras Sequential deep neural network model
# adding the deep hidden layers in the network
model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu')) 
model.add(Dense(50, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))
model.add(Dense(10, input_dim=x.shape[1], kernel_initializer='normal', activation='relu'))
model.add(Dense(1, kernel_initializer='normal'))
model.add(Dense(y.shape[1],activation='softmax')) # softmax reveals the probabilities for the classes in the network output
model.compile(loss='categorical_crossentropy', optimizer='adam')
monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')
filepath="model_{epoch:02d}-{loss:.4f}.model"
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint, monitor]
#model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=callbacks_list,verbose=2,epochs=1000)

model = load_model('best_model.model')
pred = model.predict(x_test)
pred = np.argmax(pred,axis=1)
y_eval = np.argmax(y_test,axis=1)
score = metrics.accuracy_score(y_eval, pred)
print("Validation score: {}".format(score))

